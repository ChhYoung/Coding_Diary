
# <center>机器学习实验报告<center>

  <br/><br/><br/><br/>

  <br/><br/><br/><br/>

### <center>姓名：杨崇焕
### <center>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学号：U201610531
### <center>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;班级：电信中英1601
### <center>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实验内容：贝叶斯网络

<div STYLE="page-break-after: always;"></div>

### 一.实验目的：
>使用贝叶斯网络来完成如下三个分类预测问题。
​	　任务一：使用朴素贝叶斯过滤垃圾邮件
​	　任务二：使用朴素贝叶斯对搜狗新闻语料库进行分类
​	　任务三：使用朴素贝叶斯对电影评论分类
### 二.实验原理：
> 朴素贝叶斯分类器：采用**属性条件独立性假设**：对已知类别，假设所有属性相互独立，即假设每个属性独立地分类结果结果产生影响。基于属性条件独立性假设，贝叶斯公式可以写为：
>$$p(c|\vec{x})=\frac{p(c)p(\vec{x}|c)}{p(x)}=\frac{p(c)}{p(\vec{x})}\prod_{i=0}^dp(\vec{x}|c)
$$
则朴素贝叶斯分类器：
$$
h_{nb}(x)=arg\,\max_{c\in\gamma}p(c)\prod_{i=0}^dp(x_i|c)
$$
朴素贝叶斯分类器的训练过程就是基于训练集 D 来估计类先验概率 P(c)，并为每个属性估计条件概率
### 三.实验过程：
#### &emsp;&emsp;实验环境：
> - ubuntu 18.04
>- python 3.6
>- numpy 1.14.3
>- pandas 0.23.0
>- scikit-learn 0.19.1
>- jieba
#### &emsp;&emsp;任务一：使用朴素贝叶斯过滤垃圾邮件
#### &emsp;&emsp;实验原理：
> &emsp;&emsp;**词袋模型**：用所有文本组成一个词库，单个文本的向量的值对应词库中该词出现的1位置及其在全部文本中出现的次数

#### &emsp;&emsp;实验步骤：
> **数据处理**：利用bag of words处理文本得到文本向量, 并采用交叉验证的方式从数据集中随机抽取10个作为测试集，其余40个为训练集。
> **训练**：构建一个二分类朴素贝叶斯训练函数，并计算每个属性估计条件概率
> **测试**：将测试集带入训练好的模型中即可得到结果

#### &emsp;&emsp;实验具体实现：
##### &emsp;&emsp;&emsp;1.获取文本及词库：
```python
    #获取spam，ham文本
    #将每个文本的数据作为string记录在docList
    #将类标签保存在classList中
    docList = []; classList = []
    for i in range(1, 26):
        wordList = textParse(open('./spam/%d.txt' % i, encoding="ISO-8859-1").read())
        docList.append(wordList)
        classList.append(1)

        wordList = textParse(open('./ham/%d.txt' % i, encoding="ISO-8859-1").read())
        docList.append(wordList)
        classList.append(0)
    #得到词库
    vocabList = createVocabList(docList)
```
##### &emsp;&emsp;&emsp;2.采用生成随机检索数的方法划分数据集及测试集：
```python
    #创建训练集及测试集
    trainingSet = range(50); testSet = []
    for i in range(10):
        randIndex = int(np.random.uniform(0, len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        #从原数据集中删除测试集即得到训练集
        del(list(trainingSet)[randIndex])
    #得到训练集矩阵
    trainMat = []; trainClasses = []
    for docIndex in trainingSet:
        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))
        trainClasses.append(classList[docIndex])
```
##### &emsp;&emsp;&emsp;3.构建朴素贝叶斯训练函数：通过计算每个词在词库中出现的概率来计算属性条件概率
```python
def trainNB0(trainMatrix,trainCategory):
    # number of training docs
    numTrainDocs = len(trainMatrix)
    # number of vocab in training docs
    numWords = len(trainMatrix[0])

    # p(c = 1)
    pAusive = sum(trainCategory)/float(numTrainDocs)
    # p(X|c) vector
    p0Num = np.ones(numWords) # n X 1
    p1Num = np.ones(numWords)
    #初始化概率
    p0Denom = 2.0
    p1Denom = 2.0
    #对每篇训练文档
    #   对每个类别：
    #       词条出现在文档-->增加该词条的计数
    #       增加所有词条的计数
    #   对每个类别：
    #       对每个词条：
    #           该词条数除以总词条数得到条件概率
    for i in range(numTrainDocs):
        if trainCategory[i] ==1:
            #出现词计数 
            p1Num += trainMatrix[i] 
            #计算该类别词总个数       
            p1Denom += sum(trainMatrix[i]) 
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    #对每个元素做除法
    # p(X|c) vector
    p1Vect = np.log(p1Num / p1Denom)
    p0Vect = np.log(p0Num / p0Denom)
    # 得到P(X|C=0) P(X|C=1) P(C=1)
    return p0Vect,p1Vect,pAusive
```
##### &emsp;&emsp;&emsp;4.利用朴素贝叶斯公式计算的到预测分类
```python
def classifyNB(vec2Classify,p0Vect,p1Vect,pClass1):
    p1 = sum(vec2Classify * p1Vect) + np.log(pClass1)
    p0 = sum(vec2Classify * p0Vect) + np.log(1.0 - pClass1)
    if p1 > p0:
        return 1
    else :
        return 0
```
##### &emsp;&emsp;&emsp;5.训练并得到错误率
```python
    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))
    errorCount = 0
    for docIndex in testSet:        #classify the remaining items
        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:
            errorCount += 1
            print("classification error", docList[docIndex])
    print('the error rate is: ', float(errorCount)/len(testSet))
```
##### &emsp;&emsp;&emsp;6.运行得结果
![结果截图](task1/task1_res.jpg)

#### &emsp;&emsp;任务二：使用朴素贝叶斯对搜狗新闻语料库进行分类
##### &emsp;&emsp;&emsp;1.基本实现
> **TextClassifier()实现思路:** 将输入数据转化为list()形式，然后利用sklearn中的 PredefinedSplit() 和GridSearchCV()来找出MultinomialNB()最佳参数。然后利用交叉验证得到结果.
```python
    #获取list形似的测试集，训练集，及对应标签
    train_feature_list = list(train_feature_list)
    train_class_list = list(train_class_list)
    test_feature_list = list(test_feature_list)
    test_class_list = list(test_class_list)

    X_train = train_feature_list
    Y_train  = train_class_list

    X_train_c = np.copy(train_feature_list)
    Y_train_c  = np.copy(train_class_list)
    
    X_val  = test_feature_list
    Y_val = test_class_list

    len_X_train = len(X_train)
    len_X_val = len(X_val)

    #将训练集及测试集合并以便使用GridSearchCV
    X = vstack([X_train,X_val])
    X = np.array(X)
    Y_train.extend(Y_val)
    Y = np.array(Y_train)

    #标记training-validation以便计算精度
    train_i = np.ones((len_X_train,), dtype = int) * -1
    valid_i = np.zeros((len_X_val,), dtype = int)
    split_fold = np.concatenate((train_i, valid_i))
    ps = PredefinedSplit(split_fold)

    #tuning
    param_search = GridSearchCV(classifier,
                            params, 
                    scoring=metrics.make_scorer(metrics.f1_score, average='macro'),
                                cv=ps,
                                return_train_score=True)
    param_search.fit(X,Y)
    results = param_search.cv_results_
    best_params = param_search.best_params_ 
    
    #训练及预测
    clf = MultinomialNB(alpha = best_params['alpha'])
    clf.fit(X_train_c,Y_train_c)
    Y_pred = clf.predict(X_val)
    test_accuracy = metrics.f1_score(Y_val, Y_pred, average='macro')
```
##### &emsp;&emsp;&emsp;1.1基本模型结果分析
>##### 重复划分与训练的过程得到多个结果
```python
if __name__ == '__main__':
    acc = []
    for i in range(10):
        # 文本预处理
        folder_path = './Database/SogouC/Sample'
        all_words_list, train_data_list, test_data_list, train_class_list, 
            test_class_list = TextProcessing(folder_path, test_size=0.2)

        # 生成stopwords_set
        stopwords_file = './stopwords_cn.txt'
        stopwords_set = MakeWordsSet(stopwords_file)

        # 文本特征提取和分类
        deleteN = 450
        feature_words = words_dict(all_words_list, deleteN, stopwords_set)
    
        train_feature_list, test_feature_list = TextFeatures(train_data_list, 
                            test_data_list, feature_words)

        m = TextClassifier(train_feature_list, test_feature_list, 
                    train_class_list, test_class_list)
        acc.extend([m])
    print(acc)
''' acc = [0.4464285714285714, 0.5074074074074074,   0.7555555555555555, 0.5875,
 0.6195286195286196, 0.6083333333333334, 
 0.7486772486772487, 0.5915343915343915, 0.6147186147186148, 0.825]
 '''
```
> ###### 由结果可知：对于不同的训练集及测试集划分，精度结果存在很大的差异，实验在删除特征词数量，特征词数量，训练集测试集划分比例，特征词提取方式上都存在较大改进空间，接下来将测试其中几种变量的影响